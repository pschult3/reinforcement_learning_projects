{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition - Unity Tennis Example\n",
    "# Multi-Agent Deep Deterministic Policy Gradients - MADDPG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from agent import Agent, ReplayBuffer\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set everything up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parameters\n",
    "* Unity environment\n",
    "* Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\n",
    "    'buffer_size': 1e5,\n",
    "    'batch_size': 256,\n",
    "    'n_episodes': 1200,\n",
    "    'random_episodes': 200,\n",
    "    'max_steps': 1000,\n",
    "    'update_step': 4,\n",
    "    'consecutive_update_steps': 3,\n",
    "    'solution_threshold': .5,\n",
    "    'eval_window_length': 100,\n",
    "    'num_agents': 2,\n",
    "    'agent_seed': 33,\n",
    "    'env_seed': 33,\n",
    "    'buffer_seed': 33,\n",
    "    'gamma': 0.995,\n",
    "    'tau': 1e-3,\n",
    "    'lr_actor': 1e-4,\n",
    "    'lr_critic': 1e-3,\n",
    "    'critic_weight_decay': 0,\n",
    "    'noise_sigma': 0.2,\n",
    "    'noise_scale': 2.0,\n",
    "    'noise_min_scale': 0.2,\n",
    "    'noise_scale_decay': 0.99995\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the unity environment with the tennis application\n",
    "# Set the random seed for reproduceabilty\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\",\n",
    "                       seed=params['env_seed'],\n",
    "                       no_graphics=False)\n",
    "\n",
    "# Get the default brain name from the environment\n",
    "brain_name = env.brain_names[0]\n",
    "\n",
    "# Save the brain\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment for the start\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# Get the number of agents of the specific environment\n",
    "num_agents = len(env_info.agents)\n",
    "\n",
    "# Printing the number of agents\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# Get the size of the action space\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# Printing the size of the action space\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# Get the size of the state space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "# Printing some information\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate multiple DDPG Agent and Shared Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agents\n",
    "agents = {}\n",
    "\n",
    "# For the total number of agents in the environment...\n",
    "for num_agent in range(params['num_agents']):\n",
    "    \n",
    "    # Create one agent to a specific index\n",
    "    agents[num_agent] = Agent(state_size=state_size, action_size=action_size,\n",
    "                              agent_no=num_agent, params=params)\n",
    "\n",
    "# Initialize the shared replay buffer for all agents\n",
    "replay_buffer = ReplayBuffer(params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agent with DDPG (Deep Deterministic Policy Gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the evaluation window length \n",
    "eval_window_lengths = params['eval_window_length']\n",
    "\n",
    "# Initialize that the environment is not solved\n",
    "env_is_solved = False\n",
    "\n",
    "# Initialize the scores\n",
    "scores = []\n",
    "\n",
    "# Initialize a list with a fixed length of the window-length\n",
    "scores_window = deque(maxlen=eval_window_lengths)\n",
    "\n",
    "# Get the current start time\n",
    "start = time.time()\n",
    "\n",
    "# Get the initial scaling for the exploration noise\n",
    "scale = params['noise_scale']\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(params['agent_seed'])\n",
    "\n",
    "# For the maximum number of episodes (for-loop of each episode)... \n",
    "for i_episode in range(1, params['random_episodes']+1):\n",
    "    \n",
    "    # Initialize the scores for each episode and for each agent with 0\n",
    "    episode_scores = np.zeros(params['num_agents'])\n",
    "    \n",
    "    # Initialize the best score to 0\n",
    "    best_score = 0\n",
    "    \n",
    "    # For each agent reset it...\n",
    "    for agent in agents.values():\n",
    "        \n",
    "        # Reset the current agent\n",
    "        agent.reset()\n",
    "        \n",
    "    # Get the current state of the environment\n",
    "    states = env_info.vector_observations\n",
    "    \n",
    "    # Get the current time of the episode\n",
    "    episode_start = time.time()\n",
    "    \n",
    "    # Reset the environment for the current episode\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "        \n",
    "    # Start the current episode...\n",
    "    for step in range(params['max_steps']):\n",
    "            \n",
    "        # Select uniform sampled actions\n",
    "        actions = np.random.uniform(-1, 1, (params['num_agents'], action_size))\n",
    "            \n",
    "        # Perform the actions in the environment\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "            \n",
    "        # Get the new state of all agents\n",
    "        next_states = env_info.vector_observations\n",
    "            \n",
    "        # Get the rewards \n",
    "        rewards = env_info.rewards\n",
    "            \n",
    "        # Check if a terminal state is reached\n",
    "        dones = env_info.local_done\n",
    "            \n",
    "        # Add the observation/experience to the replay buffer\n",
    "        replay_buffer.add(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "        # Add the current reward to the episode reward\n",
    "        episode_scores += rewards\n",
    "            \n",
    "        # The new state is the current state of the next iteration\n",
    "        states = next_states\n",
    "    \n",
    "        # If a terminal state is reached...\n",
    "        if np.any(dones):\n",
    "                \n",
    "            # End the current episode\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the best score to 0\n",
    "best_score = 0\n",
    "\n",
    "# Initialize the best eval scores to 0\n",
    "best_eval_score = 0\n",
    "\n",
    "# For the maximum number of episodes (for-loop of each episode)... \n",
    "for i_episode in range(1, params['n_episodes']+1):\n",
    "    \n",
    "    # Initialize the scores for each episode and for each agent with 0\n",
    "    episode_scores = np.zeros(params['num_agents'])\n",
    "    \n",
    "    # For each agent reset it...\n",
    "    for agent in agents.values():\n",
    "        \n",
    "        # Reset the current agent\n",
    "        agent.reset()\n",
    "        \n",
    "    # Get the current state of the environment\n",
    "    states = env_info.vector_observations\n",
    "    \n",
    "    # Get the current time of the episode\n",
    "    episode_start = time.time()\n",
    "    \n",
    "    # Reset the environment for the current episode\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "            \n",
    "    # Start the current episode..\n",
    "    for step in range(params['max_steps']):\n",
    "            \n",
    "        # Predict the actions with exploration noise of the current state\n",
    "        actions = [agent.act(env_info.vector_observations[no_agent], scale=scale)\n",
    "                       for no_agent, agent in enumerate(agents.values())]\n",
    "            \n",
    "        # Concatenate all actions of the current state\n",
    "        actions = np.concatenate(actions, axis=0).reshape((params['num_agents'], action_size))\n",
    "            \n",
    "        # Perform the actions in the environment\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "            \n",
    "        # Get the new state\n",
    "        next_states = env_info.vector_observations\n",
    "            \n",
    "        # Get the rewards of the performed actions\n",
    "        rewards = env_info.rewards\n",
    "            \n",
    "        # Check if a terminal state is reached\n",
    "        dones = env_info.local_done\n",
    "            \n",
    "        # Add the current observation/experience to the replay buffer\n",
    "        replay_buffer.add(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "        # Let the agents learn from its experiences\n",
    "        agent.step(replay_buffer, agents)\n",
    "            \n",
    "        # Add the current rewards to the episode rewards\n",
    "        episode_scores += rewards\n",
    "            \n",
    "        # The new state is the current state in the next iteration\n",
    "        states = next_states\n",
    "            \n",
    "        # Adjust the scale of the exploration noise by its decay\n",
    "        scale = max(params['noise_min_scale'], scale*params['noise_scale_decay'])\n",
    "            \n",
    "        # If a terminal state is reached...\n",
    "        if np.any(dones):\n",
    "                \n",
    "            # End of the episode\n",
    "            break\n",
    "    \n",
    "    # Calculate the buffer ratio by the current size and the total size\n",
    "    buffer_load_ratio = len(replay_buffer)/replay_buffer.buffer_size\n",
    "    \n",
    "    # Get the episode duration \n",
    "    episode_duration = time.time() - episode_start\n",
    "    \n",
    "    # Get the maximum episode reward\n",
    "    score = max(episode_scores)\n",
    "    \n",
    "    # Append the maximum episode reward to the evaluation buffer\n",
    "    scores.append(score)\n",
    "    \n",
    "    # Save the scores\n",
    "    #np.save('maddpg_scores', scores)\n",
    "    \n",
    "    # Append the maximum episode reward to the evaluation buffer\n",
    "    scores_window.append(score)\n",
    "    \n",
    "    # Print the current score of the episode, the buffer load ratio, the current noise scaling and the duration of the current episode\n",
    "    print('\\rEpisode {:4d} - Score: {:.2f} - Buffer Load Ratio: {:.1%} - Noise Scale: {:.2f} - Duration: {:.1f}[s]'.format(\n",
    "        i_episode, score, buffer_load_ratio, scale, episode_duration), end=\"\")\n",
    "    \n",
    "    # Every \"eval_window_length\" episodes...\n",
    "    if i_episode % params['eval_window_length'] == 0:\n",
    "            \n",
    "            # Printing the average score over \"eval_window_length\", the buffer load ratio and the total time since the start of the training\n",
    "            print('\\rEpisode {:4d} - Average Score: {:.2f} - Buffer Load Ratio: {:.1%} - Total Time since Start: {:4d}[s]'.format(\n",
    "                i_episode, np.mean(scores_window), buffer_load_ratio, int(time.time() - start)))\n",
    "    \n",
    "    # If the solution threshold is reached...\n",
    "    if (np.mean(scores_window) >= params['solution_threshold']) & (not env_is_solved):\n",
    "        \n",
    "        # Print the Average score and the current episode\n",
    "        print('\\nEnvironment solved in {:4d} episodes!\\tAverage Score: {:.2f}'.format(i_episode,\n",
    "                                                                                      np.mean(scores_window)))\n",
    "        \n",
    "        # Save that the environment is solved...\n",
    "        env_is_solved = True\n",
    "    \n",
    "    # If the environment is solved and the current score is better than the best_score...\n",
    "    if env_is_solved & (score > best_score):\n",
    "            \n",
    "            # Save the best score\n",
    "            best_score = score\n",
    "            \n",
    "            # Reset the environment\n",
    "            env_info = env.reset(train_mode=False)[brain_name]\n",
    "            \n",
    "            # Initialize the eval scores\n",
    "            episode_eval_scores = np.zeros(params['num_agents'])\n",
    "            \n",
    "            # Start the eval episode..\n",
    "            for step in range(params['max_steps']):\n",
    "    \n",
    "                # Predict the actions with exploration noise of the current state\n",
    "                actions = [agent.act(env_info.vector_observations[no_agent], scale=0)\n",
    "                               for no_agent, agent in enumerate(agents.values())]\n",
    "\n",
    "                # Concatenate all actions of the current state\n",
    "                actions = np.concatenate(actions, axis=0).reshape((params['num_agents'], action_size))\n",
    "\n",
    "                # Perform the actions in the environment\n",
    "                env_info = env.step(actions)[brain_name]\n",
    "\n",
    "                # Get the new state\n",
    "                next_states = env_info.vector_observations\n",
    "\n",
    "                # Get the rewards of the performed actions\n",
    "                rewards = env_info.rewards\n",
    "\n",
    "                # Check if a terminal state is reached\n",
    "                dones = env_info.local_done\n",
    "\n",
    "                # Add the current rewards to the episode rewards\n",
    "                episode_eval_scores += rewards\n",
    "\n",
    "                # The new state is the current state in the next iteration\n",
    "                states = next_states\n",
    "\n",
    "                # If a terminal state is reached...\n",
    "                if np.any(dones):\n",
    "\n",
    "                    # End of the episode\n",
    "                    break\n",
    "            \n",
    "            # Get the maximum of the eval episode scores\n",
    "            eval_score = max(episode_eval_scores)\n",
    "            \n",
    "            if eval_score > best_eval_score:\n",
    "                best_eval_score = eval_score\n",
    "                # For all agents...\n",
    "                for no, agent in agents.items():\n",
    "                    # Save the best actor\n",
    "                    torch.save(agent.actor_local.state_dict(), 'agent_{}_best_actor_e_{}.pth'.format(no, i_episode))\n",
    "\n",
    "                    # Save the best critic\n",
    "                    torch.save(agent.critic_local.state_dict(), 'agent_{}_best_critic_e_{}.pth'.format(no, i_episode))\n",
    "\n",
    "# Print the episode, step and maximum of scores\n",
    "print('Score (max over agents) from episode {} ({} steps): {}'.format(i_episode, step, np.max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment after running\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average scores\n",
    "average_scores = np.array([np.mean(avg_scores[i:(i+params['eval_window_length'])])\n",
    "                           for i in range(len(avg_scores)-params['eval_window_length'])])\n",
    "\n",
    "# Get the number of scores\n",
    "steps = len(avg_scores)\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "plt.plot(np.arange(steps), avg_scores, linewidth=1.5)\n",
    "plt.plot(np.arange(params['eval_window_length'], steps), average_scores, 'g-')\n",
    "plt.plot(np.arange(steps), [params['solution_threshold']]*steps, 'r-')\n",
    "plt.ylabel('Score', fontsize=16)\n",
    "plt.xlabel('Episode #', fontsize=16)\n",
    "plt.legend(['Score', 'Average Score (w = 100)', 'Solution Threshold'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Environment was solved in Episode {}!\".format(\n",
    "      np.argmax((average_scores >= params['solution_threshold']))+params['eval_window_length']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First Episode with Agent-Average Score >= 30.0: {}!\".format(\n",
    "    np.argmax(np.array(avg_scores) >= params['solution_threshold'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
